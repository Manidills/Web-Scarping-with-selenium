# -*- coding: utf-8 -*-
"""web scarping with selenium

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lYsrQpAW0Fwp2FqKt1FV7INylH5O0fz0
"""



!pip install selenium

!pip install wget

import requests , time, random
from bs4 import BeautifulSoup
from selenium import webdriver

!pip install selenium
!apt-get update # to update ubuntu to correctly run apt install
!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
import sys
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')
from selenium import webdriver
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')

browser=webdriver.Chrome('chromedriver',chrome_options=chrome_options)
browser.get('https://www.linkedin.com/uas/login')
print(browser.page_source)

file = open('/content/LinkedIn-Profile-Scrapper-in-Python/config.txt')
lines = file.readlines()
username = lines[0]
password = lines[1]


elementID = browser.find_element_by_id('username')
elementID.send_keys(username)

elementID = browser.find_element_by_id('password')
elementID.send_keys(password)

elementID.submit()

elementID = browser.find_element_by_id('input__email_verification_pin')
elementID.send_keys(904696)

elementID.submit()

print(browser.page_source)

link = 'https://www.linkedin.com/in/asaranya/'
browser.get(link)

SCROLL_PAUSE_TIME=5

last_height=browser.execute_script("return document.body.scrollHeight")

for i in range(3):
    browser.execute_script("window.scrollTo(0, document.body.scrollHeight);")

    time.sleep(SCROLL_PAUSE_TIME)

    new_height=browser.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break

    last_height=new_height

src=browser.page_source
soup=BeautifulSoup(src, 'html.parser')
soup

name_div = soup.find('div', {'class': 'flex-1 mr5'})
name_div

name_loc = name_div.find_all('ul')
name = name_loc[0].find('li').get_text().strip()
name

loc = name_loc[1].find('li').get_text().strip()
loc

profile_title = name_div.find('h2').get_text().strip()
profile_title

connection = name_loc[1].find_all('li')
connection = connection[1].get_text().strip()
connection

info = []
info.append(link)
info.append(name)
info.append(profile_title)
info.append(loc)
info.append(connection)
info



